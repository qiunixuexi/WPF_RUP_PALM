{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485bc42c",
   "metadata": {},
   "source": [
    "## 0. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset, Subset,TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class WindPowerDataset(Dataset):\n",
    "    def __init__(self, csv_file, wind_power_scaler=None, weather_scaler=None, save_scalers=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        self.data = self.data.iloc[::5, :].reset_index(drop=True)\n",
    "\n",
    "        self.original_wind_power_std = self.data.iloc[:, 2].std()\n",
    "        self.original_weather_std = self.data.iloc[:, 4:12].std()\n",
    "\n",
    "        if wind_power_scaler is None or weather_scaler is None:\n",
    "            self.wind_power_scaler = MinMaxScaler()\n",
    "            self.weather_scaler = MinMaxScaler()\n",
    "\n",
    "            self.data.iloc[:, 2] = self.wind_power_scaler.fit_transform(self.data.iloc[:, 2].values.reshape(-1, 1)).squeeze()\n",
    "\n",
    "            self.data.iloc[:, 4:12] = self.weather_scaler.fit_transform(self.data.iloc[:, 4:12])\n",
    "            \n",
    "            if save_scalers:\n",
    "                with open('wind_power_scaler.pkl', 'wb') as f:\n",
    "                    pickle.dump(self.wind_power_scaler, f)\n",
    "                with open('weather_scaler.pkl', 'wb') as f:\n",
    "                    pickle.dump(self.weather_scaler, f)\n",
    "        else:\n",
    "            self.wind_power_scaler = wind_power_scaler\n",
    "            self.weather_scaler = weather_scaler\n",
    "            self.data.iloc[:, 2] = self.wind_power_scaler.transform(self.data.iloc[:, 2].values.reshape(-1, 1)).squeeze()\n",
    "            self.data.iloc[:, 4:12] = self.weather_scaler.transform(self.data.iloc[:, 4:12])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - 312  # 288 (1440/5) + 24 (120/5)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        history_weather = self.data.iloc[idx:idx + 288, 4:12].values.astype(float)  # [288, 8]\n",
    "        wind_power_history = self.data.iloc[idx:idx + 288, 2].values.astype(float)   # [288]\n",
    "        future_weather = self.data.iloc[idx + 288:idx + 312, 4:12].values.astype(float)  # [24, 8]\n",
    "        future_wind_power = self.data.iloc[idx + 312, 2]  # float\n",
    "\n",
    "        return (\n",
    "            torch.tensor(history_weather, dtype=torch.float32),\n",
    "            torch.tensor(wind_power_history, dtype=torch.float32),\n",
    "            torch.tensor(future_weather, dtype=torch.float32),\n",
    "            torch.tensor(future_wind_power, dtype=torch.float32)\n",
    "        )\n",
    "    \n",
    "    def get_original_stds(self):\n",
    "        return {\n",
    "            'original_wind_power_std': self.original_wind_power_std,\n",
    "            'original_weather_std': self.original_weather_std.to_dict()\n",
    "        }\n",
    "    \n",
    "name='CAISO_zone_1_.csv'\n",
    "with open('weather_scaler.pkl', 'rb') as f:\n",
    "    weather_scaler = pickle.load(f)\n",
    "dataset = WindPowerDataset(name, save_scalers=True)\n",
    "wind_power_scaler=dataset.wind_power_scaler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size=32\n",
    "test_split=0.2\n",
    "test_size = int(len(dataset) * test_split)\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset = Subset(dataset, list(range(train_size)))\n",
    "test_dataset = Subset(dataset, list(range(train_size, len(dataset))))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a653414",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rup_ensemble_003_caiso.pkl', 'rb') as f:\n",
    "    rup_ensemble_003 = pickle.load(f)\n",
    "\n",
    "with open('rup_ensemble_005_caiso.pkl', 'rb') as f:\n",
    "    rup_ensemble_005 = pickle.load(f)\n",
    "\n",
    "with open('rup_ensemble_010_caiso.pkl', 'rb') as f:\n",
    "    rup_ensemble_010 = pickle.load(f)\n",
    "\n",
    "with open('RUPW010_caiso.pkl', 'rb') as f:\n",
    "    rupw_010 = pickle.load(f)\n",
    "\n",
    "with open('RUPW005_caiso.pkl', 'rb') as f:\n",
    "    rupw_005 = pickle.load(f)\n",
    "\n",
    "with open('RUPW003_caiso.pkl', 'rb') as f:\n",
    "    rupw_003 = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb56fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uap_loaded_list = []\n",
    "\n",
    "uap_loaded_list.append(rup_ensemble_003)\n",
    "uap_loaded_list.append(rup_ensemble_005)\n",
    "uap_loaded_list.append(rup_ensemble_010)\n",
    "uap_loaded_list.append(rupw_010)\n",
    "uap_loaded_list.append(rupw_005)\n",
    "uap_loaded_list.append(rupw_003)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f5c11",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading normal data from test_loader...\")\n",
    "normal_sequences = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    history_weather = batch[0].cpu().numpy()  # (B, 288, 8)\n",
    "    future_weather = batch[2].cpu().numpy()   # (B, 24, 8)\n",
    "    full_seq = np.concatenate([history_weather, future_weather], axis=1)  # (B, 312, 8)\n",
    "    normal_sequences.append(full_seq)\n",
    "\n",
    "normal_sequences = np.concatenate(normal_sequences, axis=0)  # (N, 312, 8)\n",
    "print(f\"Normal data shape: {normal_sequences.shape}\")\n",
    "\n",
    "X_normal = normal_sequences.reshape(normal_sequences.shape[0], -1).astype(np.float32)\n",
    "print(f\"Flattened normal data shape: {X_normal.shape}\")\n",
    "\n",
    "input_dim = 312 * 8  # 2496\n",
    "encoding_dim = 128   # hyperparameter, suggested 64~512\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = Autoencoder(input_dim, encoding_dim).to(device)\n",
    "\n",
    "print(\"Training Autoencoder on normal data...\")\n",
    "\n",
    "# DataLoader\n",
    "train_tensor = torch.tensor(X_normal)\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "train_loader_ae = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "autoencoder.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader_ae:\n",
    "        x = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon = autoencoder(x)\n",
    "        loss = criterion(recon, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader_ae):.6f}\")\n",
    "\n",
    "for i in range(len(uap_loaded_list)):\n",
    "    uap_loaded = uap_loaded_list[i].cpu().numpy()  # (24, 8)\n",
    "\n",
    "    # generate attacked data\n",
    "    attacked_sequences = normal_sequences.copy()\n",
    "    attacked_sequences[:, 288:, :] += uap_loaded\n",
    "    attacked_sequences = np.clip(attacked_sequences, 0.0, 1.0)\n",
    "    X_attacked = attacked_sequences.reshape(attacked_sequences.shape[0], -1).astype(np.float32)\n",
    "\n",
    "    # generate test set\n",
    "    X_test = np.concatenate([X_normal, X_attacked], axis=0)  # (2N, 2496)\n",
    "    y_test = np.concatenate([np.ones(len(X_normal)), np.zeros(len(X_attacked))], axis=0)  # normal=1, abnormal=0\n",
    "\n",
    "    # Reconstructing error as anomaly score\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.tensor(X_test).to(device)\n",
    "        recon_test = autoencoder(X_test_tensor)\n",
    "        recon_error = torch.mean((recon_test - X_test_tensor) ** 2, dim=1).cpu().numpy()  # (2N,)\n",
    "\n",
    "    # Select threshold: use 95th percentile (or 99th) of normal data\n",
    "    normal_errors = recon_error[:len(X_normal)]\n",
    "    threshold = np.percentile(normal_errors, 95)  # 90, 95, 99\n",
    "    print(f\"Reconstruction error threshold (95th percentile): {threshold:.6f}\")\n",
    "\n",
    "    # Predict: error <= threshold → normal (1), else abnormal (0)\n",
    "    y_pred = (recon_error <= threshold).astype(int)\n",
    "\n",
    "    # Evaluation indicators (positive=normal=1)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Autoencoder Anomaly Detection - UAP {i+1}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Test samples: {len(X_test)} (half normal, half attacked)\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(f\"Recon error range: [{recon_error.min():.6f}, {recon_error.max():.6f}]\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e68840",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "print(\"Loading normal data from test_loader...\")\n",
    "normal_sequences = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    history_weather = batch[0].cpu().numpy()  # (B, 288, 8)\n",
    "    future_weather = batch[2].cpu().numpy()  # (B, 24, 8)\n",
    "    full_seq = np.concatenate([history_weather, future_weather], axis=1)  # (B, 312, 8)\n",
    "    normal_sequences.append(full_seq)\n",
    "\n",
    "normal_sequences = np.concatenate(normal_sequences, axis=0)  # (N, 312, 8)\n",
    "print(f\"Normal data shape: {normal_sequences.shape}\")\n",
    "\n",
    "X_normal = normal_sequences.reshape(normal_sequences.shape[0], -1)  # (N, 2496)\n",
    "print(f\"Flattened normal data shape: {X_normal.shape}\")\n",
    "\n",
    "for i in range(len(uap_loaded_list)):\n",
    "    uap_loaded = uap_loaded_list[i].cpu().numpy()  # (24, 8)\n",
    "\n",
    "    attacked_sequences = normal_sequences.copy()\n",
    "    attacked_sequences[:, 288:, :] += uap_loaded  \n",
    "    attacked_sequences = np.clip(attacked_sequences, 0.0, 1.0)  \n",
    "\n",
    "    X_attacked = attacked_sequences.reshape(attacked_sequences.shape[0], -1)  # (N, 2496)\n",
    "\n",
    "    X_train = X_normal  \n",
    "\n",
    "    X_test = np.concatenate([X_normal, X_attacked], axis=0)  # (2N, 2496)\n",
    "\n",
    "    y_test = np.concatenate([\n",
    "        np.ones(X_normal.shape[0]),            # normal: 1 \n",
    "        np.zeros(X_attacked.shape[0])          # abnormal: 0 \n",
    "    ], axis=0)\n",
    "\n",
    "    print(\"Training Isolation Forest...\")\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.5,       \n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_samples='auto'\n",
    "    )\n",
    "    iso_forest.fit(X_train)\n",
    "\n",
    "    y_pred = iso_forest.predict(X_test) \n",
    "\n",
    "    y_pred = (y_pred == 1).astype(int)  # True if normal → 1, else 0\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)  \n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)        \n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Isolation Forest:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Test samples: {len(X_test)} (half normal, half attacked)\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} \")\n",
    "    print(f\"Recall:    {recall:.4f} \")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c1735",
   "metadata": {},
   "source": [
    "## VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc752d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def collect_training_weather_sequences(loader):\n",
    "    sequences = []\n",
    "    with torch.no_grad():\n",
    "        for _, _, weather_future, _ in loader:\n",
    "            batch_np = weather_future.cpu().numpy()  # (B, 24, 8)\n",
    "            sequences.append(batch_np)\n",
    "    sequences = np.concatenate(sequences, axis=0)  # (N, 24, 8)\n",
    "    return sequences\n",
    "\n",
    "def fit_global_var_model_on_subsequences(sequences, max_lag=2):\n",
    "\n",
    "    p = max_lag\n",
    "    all_subseq = []\n",
    "    for seq in sequences:\n",
    "        for t in range(p, 24):\n",
    "            window = seq[t - p : t + 1]  # (p+1, 8)\n",
    "            all_subseq.append(window)\n",
    "    \n",
    "    all_subseq = np.array(all_subseq)  # (M, p+1, 8)\n",
    "    M, _, k = all_subseq.shape\n",
    "    \n",
    "    Y_target = all_subseq[:, -1, :]  # (M, 8)\n",
    "    Z = all_subseq[:, :p, :]         # (M, p, 8)\n",
    "    Z_flat = Z.reshape(M, -1)        # (M, p*8)\n",
    "\n",
    "    X = np.column_stack([Z_flat, np.ones(M)])  \n",
    "    try:\n",
    "        beta, res, rank, s = np.linalg.lstsq(X, Y_target, rcond=1e-10)\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        raise RuntimeError(f\"Failed to fit VAR model: {e}\")\n",
    "\n",
    "    B = beta[:-1].reshape(p, k, k).transpose(1, 0, 2)  # (k, p, k)\n",
    "    intercept = beta[-1]\n",
    "\n",
    "    pred = Z_flat @ beta[:-1] + beta[-1]\n",
    "    residuals = Y_target - pred\n",
    "    sigma_u = np.cov(residuals, rowvar=False)\n",
    "    if sigma_u.shape == ():\n",
    "        sigma_u = np.eye(k) * 1e-6\n",
    "\n",
    "    return {\n",
    "        'B': B,\n",
    "        'intercept': intercept,\n",
    "        'sigma_u': sigma_u,\n",
    "        'lag': p,\n",
    "        'k': k\n",
    "    }\n",
    "\n",
    "def compute_var_score_for_sequence(sequence, var_params):\n",
    "    p = var_params['lag']\n",
    "    k = var_params['k']\n",
    "    if len(sequence) < p + 1:\n",
    "        raise ValueError(\"Sequence too short\")\n",
    "\n",
    "    inv_sigma = np.linalg.pinv(var_params['sigma_u'])\n",
    "    scores = []\n",
    "\n",
    "    for t in range(p, len(sequence)):\n",
    "        Z_t = sequence[t - p : t]  # (p, k)\n",
    "        Z_flat = Z_t.flatten()\n",
    "        pred = Z_flat @ var_params['B'].reshape(p * k, k) + var_params['intercept']\n",
    "        residual = sequence[t] - pred\n",
    "        maha_sq = residual @ inv_sigma @ residual.T\n",
    "        scores.append(maha_sq)\n",
    "\n",
    "    return np.mean(scores)  # Return the square of the average Mahalanobis distance\n",
    "\n",
    "def evaluate_detection_performance(var_params, test_loader, uap_tensor, alpha=0.01):\n",
    "\n",
    "    print(\"Evaluating detection performance...\")\n",
    "\n",
    "    NORMAL_LABEL = 1  \n",
    "    ANOMALY_LABEL = 0 \n",
    "\n",
    "    # Step 1: Estimate threshold on normal test data (99% confidence level)\n",
    "    normal_scores = []\n",
    "    normal_labels = []  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, weather_future, _ in test_loader:\n",
    "            weather_batch = weather_future.cpu().numpy()  # (B, 24, 8)\n",
    "            for seq in weather_batch:\n",
    "                score = compute_var_score_for_sequence(seq, var_params)\n",
    "                normal_scores.append(score)\n",
    "                normal_labels.append(NORMAL_LABEL) \n",
    "\n",
    "    normal_scores = np.array(normal_scores)\n",
    "    mean_score = normal_scores.mean()\n",
    "    std_score = normal_scores.std()\n",
    "    threshold = mean_score + 2.576 * std_score  \n",
    "    print(f\"Threshold (99%): {threshold:.4f}\")\n",
    "    print(f\"Normal scores: mean={mean_score:.4f}, std={std_score:.4f}\")\n",
    "\n",
    "    # Step 2: Collect attack sample scores\n",
    "    attacked_scores = []\n",
    "    attacked_labels = []  \n",
    "\n",
    "    uap = uap_tensor.cpu().numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, _, weather_future, _ in test_loader:\n",
    "            weather_batch = weather_future.cpu().numpy()\n",
    "            attacked_batch = np.clip(weather_batch + uap, 0, 1)  \n",
    "\n",
    "            for seq in attacked_batch:\n",
    "                score = compute_var_score_for_sequence(seq, var_params)\n",
    "                attacked_scores.append(score)\n",
    "                attacked_labels.append(ANOMALY_LABEL)  \n",
    "\n",
    "    # Step 3: Merge normal and attack samples (randomly shuffle, maintain balance)\n",
    "    all_scores = np.array(normal_scores.tolist() + attacked_scores)\n",
    "    all_labels = np.array(normal_labels + attacked_labels)\n",
    "\n",
    "    pred_labels = (all_scores <= threshold).astype(int)  \n",
    "\n",
    "    # Step 4: calculated metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    prec = precision_score(all_labels, pred_labels, pos_label=NORMAL_LABEL)\n",
    "    rec = recall_score(all_labels, pred_labels, pos_label=NORMAL_LABEL)\n",
    "    f1 = f1_score(all_labels, pred_labels, pos_label=NORMAL_LABEL)\n",
    "\n",
    "    print(f\"Detection Performance:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  Threshold: {threshold:.4f}\")\n",
    "    print(f\"  Total Samples: {len(all_labels)} (normal: {len(normal_labels)}, attacked: {len(attacked_labels)})\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1,\n",
    "        'threshold': threshold,\n",
    "        'all_labels': all_labels,\n",
    "        'pred_labels': pred_labels,\n",
    "        'all_scores': all_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6100d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collecting training sequences...\")\n",
    "train_sequences = collect_training_weather_sequences(train_loader)\n",
    "\n",
    "print(\"Fitting VAR model on subsequences...\")\n",
    "var_params = fit_global_var_model_on_subsequences(train_sequences, max_lag=2)\n",
    "\n",
    "evaluation_uaps = {\n",
    "    'RUP (0.1)': rup_ensemble_010,\n",
    "    'Simple Average (0.1)': rupw_010,\n",
    "    'RUP (0.05)': rup_ensemble_005,\n",
    "    'Simple Average (0.05)': rupw_005,\n",
    "    'RUP (0.03)': rup_ensemble_003,\n",
    "    'Simple Average (0.03)': rupw_003,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, uap_tensor in evaluation_uaps.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    result = evaluate_detection_performance(var_params, test_loader, uap_tensor, alpha=0.01)\n",
    "    results[name] = result\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Detection Performance Comparison\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Method':<20} {'Acc':<8} {'Prec':<8} {'Recall':<8} {'F1':<8}\")\n",
    "print(\"-\" * 60)\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:<20} {res['accuracy']:<8.4f} {res['precision']:<8.4f} {res['recall']:<8.4f} {res['f1_score']:<8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b7cff",
   "metadata": {},
   "source": [
    "## ODIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "print(\"Loading FUTURE_weather for Autoencoder training...\")\n",
    "sequences = []\n",
    "\n",
    "for batch in train_loader:\n",
    "    future_weather = batch[2].cpu().numpy()  \n",
    "    sequences.append(future_weather)\n",
    "\n",
    "sequences = np.concatenate(sequences, axis=0)  # (N, 24, 8)\n",
    "print(f\"Total future_weather sequences for training Autoencoder: {sequences.shape}\")\n",
    "\n",
    "X_train_tensor = torch.tensor(sequences, dtype=torch.float32)\n",
    "dataset_ae = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "dataloader_ae = DataLoader(dataset_ae, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1c7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=64, num_layers=2):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (h, c) = self.lstm(x)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size=64, output_size=8, num_layers=2):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        lstm_out, _ = self.lstm(x, hidden)\n",
    "        output = self.output_layer(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden_size=64, num_layers=2):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers)\n",
    "        self.decoder = LSTMDecoder(hidden_size, input_size, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, c) = self.encoder(x)\n",
    "        recon = self.decoder(x, (h, c))  \n",
    "        return recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMAutoencoder(input_size=8, hidden_size=64, num_layers=2).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 100\n",
    "print(\"Training Autoencoder on train_loader data...\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for data, _ in dataloader_ae:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(data)\n",
    "        loss = criterion(recon, data)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_ae):.6f}\")\n",
    "\n",
    "print(\"Autoencoder training completed!\")\n",
    "torch.save(model.state_dict(), \"autoencoder_weather_pretrained_caiso.pth\")\n",
    "print(\"Saved pre-trained autoencoder: autoencoder_weather_pretrained_caiso.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c795df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMAutoencoder(input_size=8, hidden_size=64, num_layers=2).to(device)\n",
    "model.load_state_dict(torch.load(\"autoencoder_weather_pretrained_caiso.pth\"))\n",
    "model.eval()  \n",
    "print(\"Loaded pre-trained autoencoder for ODIN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2dcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epsilon = 0.01\n",
    "alpha = 0.01\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def odin_score_ae(x, model, T=1000, epsilon=0.01, device='cuda'):\n",
    "    model.train()\n",
    "    x = x.to(device).requires_grad_()\n",
    "    recon = model(x)\n",
    "    loss = F.mse_loss(recon, x, reduction='sum')  \n",
    "    grad = torch.autograd.grad(loss, x, create_graph=False)[0]\n",
    "    x_perturbed = x - epsilon * grad.sign()\n",
    "    recon_pert = model(x_perturbed)\n",
    "    perturbed_loss = F.mse_loss(recon_pert, x_perturbed, reduction='none').mean(dim=[1,2])\n",
    "    odin_scores = -perturbed_loss.cpu().detach().numpy()\n",
    "    return odin_scores\n",
    "\n",
    "normal_data_list = [sample[2] for sample in test_dataset]\n",
    "normal_data = torch.stack(normal_data_list).float().to(device)\n",
    "\n",
    "all_normal_scores = []\n",
    "for i in range(0, len(normal_data), batch_size):\n",
    "    batch = normal_data[i:i+batch_size]\n",
    "    scores = odin_score_ae(batch, model, epsilon=epsilon, device=device)\n",
    "    all_normal_scores.extend(scores)\n",
    "normal_scores = np.array(all_normal_scores)\n",
    "\n",
    "threshold = np.percentile(normal_scores, alpha * 100)  \n",
    "print(f\"Threshold: {threshold:.6f} (keep 99% normal samples)\")\n",
    "\n",
    "for uap_idx, uap_noise in enumerate(uap_loaded_list):\n",
    "    uap_noise = uap_noise.to(device)\n",
    "    attacked_data = torch.clamp(normal_data + uap_noise, 0, 1)\n",
    "\n",
    "    all_attacked_scores = []\n",
    "    for i in range(0, len(attacked_data), batch_size):\n",
    "        batch = attacked_data[i:i+batch_size]\n",
    "        scores = odin_score_ae(batch, model, epsilon=epsilon, device=device)\n",
    "        all_attacked_scores.extend(scores)\n",
    "    attacked_scores = np.array(all_attacked_scores)\n",
    "\n",
    "    scores_all = np.concatenate([normal_scores, attacked_scores])\n",
    "    labels_all = np.concatenate([\n",
    "        np.ones(len(normal_scores)),      \n",
    "        np.zeros(len(attacked_scores))    \n",
    "    ])\n",
    "\n",
    "    # AUC: The higher the score, the more normal it is → Normal is a positive example → Labels=1 → Directly use scores\n",
    "    auc = roc_auc_score(labels_all, scores_all)\n",
    "\n",
    "    pred_labels = (scores_all >= threshold).astype(int)  \n",
    "\n",
    "    acc = accuracy_score(labels_all, pred_labels)\n",
    "    prec = precision_score(labels_all, pred_labels, pos_label=1)\n",
    "    rec = recall_score(labels_all, pred_labels, pos_label=1)  \n",
    "    f1 = f1_score(labels_all, pred_labels, pos_label=1)\n",
    "\n",
    "    fpr = 1 - (pred_labels[:len(normal_scores)] == 1).mean()\n",
    "    tnr = (pred_labels[len(normal_scores):] == 0).mean()  \n",
    "\n",
    "    print(f\"[UAP {uap_idx}] AUC: {auc:.4f}, Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    print(f\"             Normal:  {normal_scores.mean():.6f} ± {normal_scores.std():.6f}\")\n",
    "    print(f\"             Attacked: {attacked_scores.mean():.6f} ± {attacked_scores.std():.6f}\")\n",
    "    print(f\"             Threshold: {threshold:.6f} (α={alpha:.3f})\")\n",
    "    print(f\"             Precision: {prec:.6f}, Recall: {rec:.6f}\")\n",
    "    #print(f\"             FPR: {fpr:.6f}, Specificity: {tnr:.6f}\")\n",
    "\n",
    "model.eval()\n",
    "print(\"\\n✅ Model set back to eval mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83374bde",
   "metadata": {},
   "source": [
    "## 3-sigma and boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf66fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(name).iloc[::5, 4:12]\n",
    "max_value = data.max()\n",
    "min_value = data.min()\n",
    "q1 = data.quantile(0.25)\n",
    "q3 = data.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "lower_bound_array = lower_bound.values\n",
    "upper_bound_array = upper_bound.values\n",
    "weather_stds = dataset.get_original_stds()\n",
    "weather_stds_array = np.array(list(weather_stds['original_weather_std'].values()))  \n",
    "weather_data = pd.read_csv(name)\n",
    "weather_data = weather_data.iloc[::5, :].reset_index(drop=True)\n",
    "weather_mean = weather_data.iloc[:, 4:12].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_attack_all(uap_loaded, weather_mean, lower_bound_array, upper_bound_array, weather_stds_array, weather_scaler, data_loader, device='cpu'):\n",
    "    lower_bound = torch.tensor(lower_bound_array).to(device)\n",
    "    upper_bound = torch.tensor(upper_bound_array).to(device)\n",
    "    weather_mean = torch.tensor(weather_mean).to(device)\n",
    "    weather_stds = torch.tensor(weather_stds_array).to(device)\n",
    "\n",
    "    NORMAL_LABEL = 1\n",
    "    ANOMALY_LABEL = 0\n",
    "\n",
    "    y_true_xigema = []\n",
    "    y_pred_xigema = []\n",
    "    y_true_xiang = []\n",
    "    y_pred_xiang = []\n",
    "\n",
    "    for history_weather, wind_history, weather_future, future_wind_power in data_loader:\n",
    "        B, T, C = weather_future.shape  # batch_size, seq_len, channels\n",
    "\n",
    "        weather_future = weather_future.to(device)\n",
    "\n",
    "        adversarial_weather = weather_future + uap_loaded\n",
    "        adversarial_weather = torch.clamp(adversarial_weather, 0, 1)\n",
    "\n",
    "        normal_inv = weather_scaler.inverse_transform(\n",
    "            weather_future.reshape(-1, 8).detach().cpu().numpy()\n",
    "        ).reshape(B, T, C)\n",
    "\n",
    "        adv_inv = weather_scaler.inverse_transform(\n",
    "            adversarial_weather.reshape(-1, 8).detach().cpu().numpy()\n",
    "        ).reshape(B, T, C)\n",
    "\n",
    "        normal_inv = np.array(normal_inv)\n",
    "        adv_inv = np.array(adv_inv)\n",
    "\n",
    "        for i in range(B):\n",
    "            normal_sample = normal_inv[i]  \n",
    "            adv_sample = adv_inv[i]       \n",
    "\n",
    "            mean_np = weather_mean.cpu().numpy()\n",
    "            stds_np = weather_stds.cpu().numpy()\n",
    "            lower_3sigma = mean_np - 3 * stds_np\n",
    "            upper_3sigma = mean_np + 3 * stds_np\n",
    "\n",
    "            if np.any((normal_sample < lower_3sigma) | (normal_sample > upper_3sigma)):\n",
    "                pred_normal_xigema = ANOMALY_LABEL  \n",
    "            else:\n",
    "                pred_normal_xigema = NORMAL_LABEL  \n",
    "\n",
    "            if np.any((adv_sample < lower_3sigma) | (adv_sample > upper_3sigma)):\n",
    "                pred_adv_xigema = ANOMALY_LABEL \n",
    "            else:\n",
    "                pred_adv_xigema = NORMAL_LABEL  \n",
    "\n",
    "            y_true_xigema.extend([NORMAL_LABEL, ANOMALY_LABEL])  \n",
    "            y_pred_xigema.extend([pred_normal_xigema, pred_adv_xigema])\n",
    "\n",
    "            if np.any((normal_sample < lower_bound_array) | (normal_sample > upper_bound_array)):\n",
    "                pred_normal_xiang = ANOMALY_LABEL \n",
    "            else:\n",
    "                pred_normal_xiang = NORMAL_LABEL  \n",
    "\n",
    "            if np.any((adv_sample < lower_bound_array) | (adv_sample > upper_bound_array)):\n",
    "                pred_adv_xiang = ANOMALY_LABEL  \n",
    "            else:\n",
    "                pred_adv_xiang = NORMAL_LABEL   \n",
    "\n",
    "            y_true_xiang.extend([NORMAL_LABEL, ANOMALY_LABEL])  \n",
    "            y_pred_xiang.extend([pred_normal_xiang, pred_adv_xiang])\n",
    "\n",
    "    y_true_xigema = np.array(y_true_xigema)\n",
    "    y_pred_xigema = np.array(y_pred_xigema)\n",
    "    y_true_xiang = np.array(y_true_xiang)\n",
    "    y_pred_xiang = np.array(y_pred_xiang)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"3 sigma:\")\n",
    "    print(\"=\"*50)\n",
    "    acc_xigema = accuracy_score(y_true_xigema, y_pred_xigema)\n",
    "    prec_xigema = precision_score(y_true_xigema, y_pred_xigema)  # P = TP / (TP + FP)\n",
    "    rec_xigema = recall_score(y_true_xigema, y_pred_xigema)      # R = TP / (TP + FN)\n",
    "    f1_xigema = f1_score(y_true_xigema, y_pred_xigema)\n",
    "\n",
    "    print(f\"Accuracy:  {acc_xigema:.4f}\")\n",
    "    print(f\"Precision: {prec_xigema:.4f}\")\n",
    "    print(f\"Recall:    {rec_xigema:.4f}\")\n",
    "    print(f\"F1 Score:  {f1_xigema:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Boxplot:\")\n",
    "    print(\"=\"*50)\n",
    "    acc_xiang = accuracy_score(y_true_xiang, y_pred_xiang)\n",
    "    prec_xiang = precision_score(y_true_xiang, y_pred_xiang)\n",
    "    rec_xiang = recall_score(y_true_xiang, y_pred_xiang)\n",
    "    f1_xiang = f1_score(y_true_xiang, y_pred_xiang)\n",
    "\n",
    "    print(f\"Accuracy:  {acc_xiang:.4f}\")\n",
    "    print(f\"Precision: {prec_xiang:.4f}\")\n",
    "    print(f\"Recall:    {rec_xiang:.4f}\")\n",
    "    print(f\"F1 Score:  {f1_xiang:.4f}\")\n",
    "\n",
    "    return {\n",
    "        '3sigma': {\n",
    "            'acc': acc_xigema,\n",
    "            'prec': prec_xigema,\n",
    "            'rec': rec_xigema,\n",
    "            'f1': f1_xigema\n",
    "        },\n",
    "        'boxplot': {\n",
    "            'acc': acc_xiang,\n",
    "            'prec': prec_xiang,\n",
    "            'rec': rec_xiang,\n",
    "            'f1': f1_xiang\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uap_loaded in uap_loaded_list:\n",
    "    adversarial_attack_all(uap_loaded,weather_mean,lower_bound_array,upper_bound_array,weather_stds_array, weather_scaler, test_loader, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-zh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
